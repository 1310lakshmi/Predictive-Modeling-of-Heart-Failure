# -*- coding: utf-8 -*-
"""Machine Learning-Project: Group-16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SuzRVilp953oHkOpiG2Vadd3HImwBNRP

#Predictive Modeling of Heart Failure:
**A
Comprehensive Exploration of Machine Learning
Classification Models**

Team member (Group 16):
1.  Lakshmi Aparna Chukka - 2197593

# Import the libraries and dataset

Importing necessary packages
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import stats

from sklearn import metrics
from sklearn.metrics import log_loss,confusion_matrix
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.model_selection import GridSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
import plotly.express as px

import warnings
warnings.filterwarnings('ignore')

"""**Importing the dataset**"""

df= pd.read_csv('heart.csv')
df.head()

df.shape

"""# Data exploration

**Information about data**
"""

df.info()

"""There are no NULL values in the dataset.

'FastingBS' and 'HeartDisease' have only two values, so we change them to categorial datatype for better analysis.

Numerical features are correctly defined with their respective datatypes, so there are no possiblities for improper/special characters in numerical features.

**Checking for Duplicate entries in the dataset**
"""

Duplicate = df[df.duplicated]

print("Duplicate Entries :")

Duplicate.shape

"""There are no duplicate entries in the dataset

Converting numerical features having only two values to categorical features for better analysis
"""

df['FastingBS']=df['FastingBS'].astype('category')
df['HeartDisease']=df['HeartDisease'].astype('category')

"""**Segregating categorical features and numerical features**"""

num_cols=df.select_dtypes(include=['int64','float64']).columns.tolist()

cat_cols=df.select_dtypes(include=['category','object']).columns.tolist()

print(num_cols)
print(cat_cols)

"""**Checking for Values in each categorical feature and looking for any special/improper characters**"""

for i in cat_cols:
     print("{} : {} ".format(i,df[i].unique()))

"""There are no special/improper characters present in categorical features"""

df.describe().T

"""**Insights**

mean and median of "Age" is almost equal - Normally distributed

mean and median of "RestingBP" is similar - Normally distributed

mean of "Cholestrol" is lesser than its median - Left skewed

mean and median of "MaxHR" is almost equal - Normally distributed

mean of "Oldpeak" is greater than its median - Right skewed

# Visualization of data
"""

px.histogram(df,x='Age',color='HeartDisease', nbins=60).update_layout(title='Age Distribution')

df_sex_heart = df.groupby(['Sex','HeartDisease'])['Sex'].count().reset_index(name='Total')
sns.barplot(data=df_sex_heart,x='Sex',y='Total',hue='HeartDisease')
plt.title('Total People by Sex and HeartDisease');

px.histogram(data_frame=df,x='RestingBP',color='HeartDisease',nbins=60).update_layout(title='RestingBP Distribution')

px.histogram(data_frame=df,x='Cholesterol',color='HeartDisease',nbins=60).update_layout(title='Cholestrol Distribution')

px.histogram(data_frame=df,x='MaxHR',color='HeartDisease',nbins=100).update_layout(title='Max Heart Rate Distribution')

df_agina_heart = df.groupby(['ExerciseAngina','HeartDisease'])['ExerciseAngina'].count().reset_index(name='Total')
sns.barplot(data=df_agina_heart,x='ExerciseAngina',y='Total',hue='HeartDisease')
plt.title('Total People by ExerciseAngina and HeartDisease');

px.histogram(data_frame=df,x='Oldpeak',color='HeartDisease').update_layout(title='Oldpeak Distribution')

target_values = df['HeartDisease'].value_counts()

plt.pie(target_values, labels=['Heart Disease', 'Normal'],autopct='%.2f%%',
       startangle=240, colors=['red','green'], wedgeprops={'edgecolor':'k','linewidth':3,'antialiased' : True})
plt.title('Spread of Heart Disease')
plt.tight_layout()
plt.show()

"""# Data Analysis

**Univariate analysis**

Univariate analysis on numerical features
"""

for i in num_cols:
    print('{} \n\n {}'.format('\033[1m',i.upper()))
    f, axes = plt.subplots(1, 2, figsize=(15,5))
    sns.boxplot(x = i, data=df,  orient='h' , ax=axes[1])
    sns.distplot(df[i],  ax=axes[0])
    axes[0].set_title('Distribution plot')
    axes[1].set_title('Box plot')
    plt.show()


    q25,q75=np.percentile(df[i],25),np.percentile(df[i],75)
    IQR=q75-q25
    Threshold=IQR*1.5
    lower,upper=q25-Threshold,q75+Threshold
    Outliers=[i for i in df[i] if i<lower or i>upper]
    print('Total number of outliers in {}: {}'.format(i,len(Outliers)))
    percent=round((len(Outliers)/len(df[i]))*100,2)
    print('Percentage of outliers in {}: {} %'.format(i,percent))

"""There are some outliers in the dataset, we need to handle them before building the model.

Also we can see that cholestrol level of some people is zero, which should be handled before building the model because cholestrol levels can never be zero this maybe because of some human errors.

**Bivariate Analysis**

Numerical variables vs Target variable
"""

for i in num_cols:
    if i != 'HeartDisease':
      f,axes=plt.subplots(1,2,figsize=(15,6))
      sns.boxplot(x='HeartDisease', y=i, data= df, ax=axes[0])
      ax=df.groupby(by=['HeartDisease'])[i].mean().reset_index().sort_values(i,ascending=True).plot(x='HeartDisease',y=i,kind='bar',ax=axes[1])

      for p in ax.patches:
          ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))

"""**Insights**
- People with the average age of about 56 have high risk of being affected by heart disease

- Men tend to have high risk of being affected by heart disease as compared to women

- RestingBP does not provide any clear conclusion on heartdisease as their averages are almost similar

- People having high cholestrol levels have high risk of being affected by heart disease

- People with heart diseases can reach the maximum heart rate of 127(on average) which is lesser than the maximum heart rate of people with no heart disease

- oldpeak = ST [Numeric value measured in depression] - People having large oldpeak(large oldpeak = depression)  
have high risk of being affected by heart disease

**Multivariate analysis**
"""

sns.pairplot(df);

"""**Insights**

Cholestrol and RestingBP with value 0 should be handled before building the model

# Data preprocessing

**1. Dealing with incorrect values**
"""

df['RestingBP'] = df['RestingBP'].replace(0,df['RestingBP'].mean())
before_corr = round(df['Cholesterol'].corr(df['HeartDisease']),3)
drop_corr = round(df['Cholesterol'].replace(0, np.nan).dropna().corr(df['HeartDisease']),3)
mean_corr = round(df['Cholesterol'].replace(0, np.nan).fillna(df['Cholesterol'].mean()).corr(df['HeartDisease']),3)
median_corr = round(df['Cholesterol'].replace(0, np.nan).fillna(df['Cholesterol'].median()).corr(df['HeartDisease']),3)

corr_df = pd.DataFrame(data=[[before_corr, drop_corr,mean_corr,median_corr]], columns=['before_corr','corr_after_drop','corr_after_mean','corr_after_median'])
corr_df

"""1.  Since zero RestingBP values and zero Cholesterol values are practically impossible, we update the RestingBP value with the mean value of the respective features.
2.  For the Cholesterol values, we tried several methods (drop, fill
mean, fill median). However, dropping all zero values gave us the best
result.

**2. Replacing outliers with NaN- Replacing outtliers by treating them as missing values**
"""

for c in num_cols:
    #getting upper lower quartile values
    q25,q75=np.percentile(df[c],25),np.percentile(df[c],75)
    IQR=q75-q25
    Threshold=IQR*1.5
    lower,upper=q25-Threshold,q75+Threshold

    df[c]=np.where(df[c]>upper,np.nan,df[c])
    df[c]=np.where(df[c]<lower,np.nan,df[c])

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp=IterativeImputer(missing_values=np.nan)
idf=pd.DataFrame(imp.fit_transform(df[num_cols]))
idf.columns=df[num_cols].columns
idf.index=df.index
df[['RestingBP','Cholesterol','MaxHR','Oldpeak']]=idf[['RestingBP','Cholesterol','MaxHR','Oldpeak']]

"""**3. Segregating Predictors vs Target feature**"""

X=df.drop(columns="HeartDisease")
y=df["HeartDisease"]

"""Since there are no Ordinal categorical features we can go for One-Hot encoding

**4. One hot encoding**
"""

X=pd.get_dummies(X,drop_first=True)

"""**5. Scaling the continuous features and applying the scaled data to the dataframe X**"""

scaler = StandardScaler()
temp=X[num_cols] #Temporary dataframe
temp[num_cols]= scaler.fit_transform(temp)

X[num_cols]=temp
X[['Age','RestingBP','Cholesterol','MaxHR','Oldpeak']]=temp
X.head()

new_df=X.join(y,how='inner')
new_df.to_csv("clean_dataset")

"""# Model training, testing and performance comparison

**Splitting the dataset into train and test datasets**
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10,stratify = y)

def fit_models(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)

    pred = model.predict(X_test)

    accuracy = metrics.accuracy_score(y_test,pred)

    precision = metrics.precision_score(y_test,pred)

    recall = metrics.recall_score(y_test,pred)

    f1 = metrics.f1_score(y_test,pred)

    loss = metrics.log_loss(y_test,pred)

    scorer = {'accuracy' : metrics.make_scorer(metrics.accuracy_score),
              'precision' : metrics.make_scorer(metrics.precision_score),
              'recall' : metrics.make_scorer(metrics.recall_score),
              'f1' : metrics.make_scorer(metrics.f1_score),
              'loss' : metrics.make_scorer(metrics.log_loss)
             }
    cv = cross_validate(model, X_train, y_train, cv=10, scoring = scorer)

    accuracy_cv = cv['test_accuracy'].mean()
    precision_cv = cv['test_precision'].mean()
    recall_cv = cv['test_recall'].mean()
    f1_cv = cv['test_f1'].mean()
    loss_cv = cv['test_loss'].mean()

    return accuracy, precision, recall, f1, loss, accuracy_cv, precision_cv,recall_cv,f1_cv,loss_cv

"""Here we analyse and compare the performance of various models such as logistic regression, K nearest neighbour and SVM."""

lg = LogisticRegression()
dt = DecisionTreeClassifier()
knc = KNeighborsClassifier()
svm = SVC()

result = {}

for model,name in zip([lg, dt, knc, svm],
                     ['Logistic Regression',
                     'Decision Tree', 'K-Nearest Neighbour', 'SVM']):
    result[name] = fit_models(model,X_train, X_test,y_train, y_test)

result_table = pd.DataFrame(np.array(list(result.values())),    # make a dataframe out of the metrics from result dictionary
                       columns= ['ACCURACY', 'PRECISION', 'RECALL', 'F1-SCORE', 'LOG LOSS',
                                 'ACCURACY CV', 'PRECISION CV', 'RECALL CV', 'F1-SCORE CV', 'LOG LOSS CV'],
                      index= result.keys())   # use the model names as index

result_table.index.name = 'Model'   # name the index of the result_table dataframe as 'Model'

result_table

"""From the above results we notice that Logistic Regression and SVM have higher Accuracy and F1-SCORE, we shall perform hyperparameter tuning on these 2 models to see if we can further increase the performance.

#  Ensemble Models
Here we analyse and compare the performance of various ensemble models such as random forest, bagging, boosting, gradient boosting.
"""

rfcl = RandomForestClassifier(n_estimators = 50, random_state=1,max_features=12)
bgcl = BaggingClassifier(n_estimators=10,random_state=1)
abcl = AdaBoostClassifier(n_estimators=10, random_state=1)
gbcl = GradientBoostingClassifier(n_estimators = 50,random_state=1)



result = {}

for model,name in zip([rfcl, bgcl, abcl, gbcl],
                     ['Random Forest Classifier','Bagging Classifier', 'Adaptive Boosting',
                      'Gradient Boosting']):
    result[name] = fit_models(model,X_train, X_test,y_train, y_test)

result_table = pd.DataFrame(np.array(list(result.values())),    # make a dataframe out of the metrics from result dictionary
                       columns= ['ACCURACY', 'PRECISION', 'RECALL', 'F1-SCORE', 'LOG LOSS',
                                 'ACCURACY CV', 'PRECISION CV', 'RECALL CV', 'F1-SCORE CV', 'LOG LOSS CV'],
                      index= result.keys())   # use the model names as index

result_table.index.name = 'Model'   # name the index of the result_table dataframe as 'Model'

result_table

"""From the above results we notice that Gradient Boosting has higher Accuracy and F1-SCORE, we shall perform hyperparameter tuning on this model to see if we can further increase the performance.

# Hyper-parameter Tuning

Finally, we peform hyper-parameter tuning on the best performing models.

**Hyperparameter tuning-Logistic Regression**
"""

Log_clf = GridSearchCV(LogisticRegression(),{
    'C' : [100, 10, 1.0, 0.1, 0.01],
    'solver' : ['newton-cg', 'lbfgs', 'liblinear'],
    'penalty' : ['l1','l2','elasticnet']
}, cv=10, return_train_score=False)

Log_clf_fit=Log_clf.fit(X,y)
print(Log_clf_fit.best_score_)
print(Log_clf_fit.best_params_)

"""**Logistic Regression with the best chosen parameters**"""

logit = LogisticRegression(C = 1.0, penalty='l2', solver = 'liblinear')
logit.fit(X_train, y_train)
print('Accuracy on Training data:',logit.score(X_train, y_train) )
print('Accuracy on Test data:',logit.score(X_test, y_test) )

logit_pred = logit.predict(X_test)
print("\n Classification  Report:\n",classification_report(y_test,logit_pred))
# print("f1_score of Logistic Regression: ",metrics.f1_score(y_test,logit_pred))

"""**Hyperparameter tuning-SVM**"""

svc_clf = GridSearchCV(SVC(),{
    'C' : [100,10,1,0.1,0.01,0.001],
    'gamma' : [100,10,0.1,0.01,0.001],
    'kernel' : ['rbf','linear','poly']
}, cv=10, return_train_score=False)

svc_clf_fit=svc_clf.fit(X,y)
print(svc_clf_fit.best_score_)
print(svc_clf_fit.best_params_)

"""**SVM with the best chosen parameters**"""

svc = SVC(kernel = 'rbf', C = 1, gamma = 0.1)
svc.fit(X_train,y_train)
print('Accuracy on Training data: ',svc.score(X_train,y_train))
print('Accuracy on Test data:',svc.score(X_test,y_test))

predicted=svc.predict(X_test)
print("\n Classification  Matrix:\n",classification_report(y_test,predicted))

# print("f1_score of SVC: ",metrics.f1_score(y_test,predicted))

"""**Hyperparameter tuning - Gradient Boosting**"""

gdb_clf = GridSearchCV(GradientBoostingClassifier(),{
    'n_estimators' : [10, 100, 1000],
    'learning_rate' : [0.001, 0.01, 0.1],
    'subsample' : [0.5, 0.7, 1.0],
    # 'max_depth' : [3,7,9]
}, cv=10, return_train_score=False)

gdb_clf_fit=gdb_clf.fit(X,y)
print(gdb_clf_fit.best_score_)
print(gdb_clf_fit.best_params_)

"""**Gradient Boosting with the best chosen parameters**"""

gbcl = GradientBoostingClassifier(n_estimators = 1000, max_depth = 3, learning_rate = 0.01, subsample = 0.7, random_state = 1)
gbcl = gbcl.fit(X_train, y_train)
print("Ensemble learning(GradientBoosting) Training data score: ",gbcl.score(X_train,y_train))
print("Ensemble learning(GradientBoosting) Testing data score: ",gbcl.score(X_test,y_test))

y_predict = gbcl.predict(X_test)
print("classification  Matrix:\n",classification_report(y_test,y_predict))

# print("f1_score of Gradient boost: ",metrics.f1_score(y_test,y_predict))

"""**Conclusion:**


1.   Linear regression, Support Vector Machines and Gradient Boosting are the best performing heart failure detection models.

2. Gradient boosting algorithm demonstrated an accuracy of 89.13% on the test data.

3.    After hyper-parameter tuning we find that Support Vector Machines also perform with an accuracy of 89.13 % on testing data

4.  This underscores the significance of fine-tuning model parameters in achieving superior performance in predicting heart failure.





"""